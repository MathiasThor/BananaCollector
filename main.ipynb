{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Banana Collector\n",
    "\n",
    "---\n",
    "The work is about training an agent using deep reinforcement learning to navigate and collect bananas in a large, square world. A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The following code is adapted from Udacitys Deep Reinforement Learning programme.\n",
    "Please see readme for instructions on how to run the code.\n",
    "\n",
    "## Setup envionment and deep Q-learning agent\n",
    "Import libaries and setup environment and DQL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries \n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load unity banana environment\n",
    "env = UnityEnvironment(file_name=\"./unity_simulation/Banana.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Make agent to learn the task. This agent implements DQL\n",
    "from banana_agent import Agent\n",
    "agent = Agent(state_size=37, action_size=4, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent\n",
    "Training loop that uses the agent to take actions and update the deep Q-network. The loop makes use of the banana environment to collect rewards, observations, and restart the simulation when necessary. Finally, the scores and learned weights are collected and saved to files that can be used for documentation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.015, eps_decay=0.0027):\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "        \n",
    "    # Run episodes\n",
    "    for i_episode in range(1, n_episodes+1):               # Run for n_episodes episodes\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # Restart the env\n",
    "        state = env_info.vector_observations[0]            # Get current state\n",
    "        score = 0                                          # Initialize score to 0 \n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                 # Get action from agent\n",
    "            env_info = env.step(action)[brain_name]        # Send action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # Get the next state\n",
    "            reward = env_info.rewards[0]                   # Get the reward\n",
    "            done = env_info.local_done[0]                  # See if episode has finished\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done) # Step agent (i.e., train)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score      \n",
    "        eps = max(eps_end, eps-eps_decay) # linear decay epsilon\n",
    "        \n",
    "        # Print output to user\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.3f}'.format(i_episode, np.mean(scores_window), eps), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.3f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "        if np.mean(scores_window)>=20.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            break\n",
    "    \n",
    "    # Save trained model\n",
    "    torch.save(agent.qnetwork_local.state_dict(), './saved_models/dense_model.pth')\n",
    "    return scores\n",
    "\n",
    "episodes_to_run = 1000 # Number of episodes per trial \n",
    "index_collector = range(0, episodes_to_run) # Index used for plotting\n",
    "trials = 1 # Number of training trials (used for plotting) \n",
    "\n",
    "for i in range(trials):\n",
    "    print(\"\\n -- Trial {} --\".format(i+1))\n",
    "    if i == 0:\n",
    "        scores_current = dqn(episodes_to_run) # Run training algorithm\n",
    "        scores = np.vstack((range(0, episodes_to_run), scores_current)).T # Save scores\n",
    "    else:\n",
    "        scores_current = dqn(episodes_to_run) # Run training algorithm\n",
    "        scores = np.concatenate((scores, np.vstack((index_collector, scores_current)).T)) # Save scores\n",
    "\n",
    "# df = pd.DataFrame(scores) \n",
    "# df.to_csv('./data/scores_sparse.csv', index=False) # Save training results (scores) to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Plotter\n",
    "Plotter function that can plot the scores file generated from using the above training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Read and postprocess CSV file generated while training\n",
    "def load_csv(csv_name):\n",
    "    # Get and arrange data\n",
    "    df_scores = pd.read_csv(csv_name)   # Read CSV file generated while training\n",
    "    df_scores.columns = [\"episode\", \"score\"]      # Rename columns \n",
    "    df_scores_smooth = df_scores.groupby([\"episode\"], as_index=False).mean()        # Prepare for rolling mean\n",
    "    df_scores_smooth['smooth_score'] = df_scores_smooth['score'].rolling(15).mean() # Use rolling mean of 15\n",
    "    return df_scores, df_scores_smooth\n",
    "\n",
    "# Setup seaborn for plotting\n",
    "sns.set(\"talk\", rc={\"lines.linewidth\": 3, 'figure.figsize': (14, 7)})\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Get and arrange data\n",
    "df_scores_sparse, df_scores_smooth_sparse = load_csv(\"./data/scores_sparse.csv\")   \n",
    "df_scores_dense, df_scores_smooth_dense = load_csv(\"./data/scores_dense.csv\")   \n",
    "\n",
    "# Plot data: first raw score with mean and std error - then the rolling mean with a darker color\n",
    "sns_plot = sns.lineplot('episode', 'score', data=df_scores_dense, linewidth = 1, color=\"navy\", alpha = 0.5, label='_nolegend_')\n",
    "sns_plot = sns.lineplot('episode', 'smooth_score', data=df_scores_smooth_dense, linewidth = 3, color=\"navy\")\n",
    "sns_plot = sns.lineplot('episode', 'score', data=df_scores_sparse, linewidth = 1, color=\"orange\", alpha = 0.5, label='_nolegend_')\n",
    "sns_plot = sns.lineplot('episode', 'smooth_score', data=df_scores_smooth_sparse, linewidth = 3, color=\"orange\")\n",
    "\n",
    "sns_plot.axhline(13, ls='--', color=\"green\", linewidth=5, alpha=0.75)  # Insert line at 13 which is the goal \n",
    "sns_plot.set(ylabel='Score', xlabel='Episode #')\n",
    "sns_plot.legend(fontsize = 'medium', loc='upper left', labels=[\"Dense DQN\", \"Sparse DQN\"], framealpha = 0)\n",
    "\n",
    "figure = sns_plot.get_figure()\n",
    "figure.tight_layout()\n",
    "figure.savefig('./data/score_DQN.png', dpi=200) # Save figure to score.png\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Viewer\n",
    "Run the trained agent in the banana collector environment using the saved weights from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "from banana_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=37, action_size=4, seed=time.time())      # Make agent\n",
    "agent.qnetwork_local.load_state_dict(torch.load('./saved_models/sparse_model.pth')) # load weights from previous training session\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./unity_simulation/Banana.x86_64\") # Load banana environment. !Remember to change network in banana_agent.py\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]  # reset the environment\n",
    "state = env_info.vector_observations[0]             # get the current state\n",
    "score = 0                                           # initialize the score\n",
    "while True:\n",
    "    action = agent.act(state, 0)                    # select an action\n",
    "    env_info = env.step(action)[brain_name]         # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]    # get the next state\n",
    "    reward = env_info.rewards[0]                    # get the reward\n",
    "    done = env_info.local_done[0]                   # see if episode has finished\n",
    "    score += reward                                 # update the score\n",
    "    state = next_state                              # roll over the state to next time step\n",
    "    if done:                                        # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
